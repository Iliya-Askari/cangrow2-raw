# Resume and Job Description Matching Model

This repository contains an end-to-end natural language processing (NLP) pipeline that matches free-form resume text to the most relevant job title and highlights the most important skills missing from the resume. The workflow is implemented in a Jupyter notebook and produces a serialized scikit-learn model bundle that can be reused in other applications.

The project is intended for:

- Career platforms that want to automatically classify resumes and recommend upskilling paths.
- Job seekers interested in understanding how their current skills align with real job-market expectations.
- Data practitioners looking for a clean, reproducible example of text classification + keyword gap analysis.

---

## üì¶ Repository Contents

```text
.
‚îú‚îÄ‚îÄ DataSet/
‚îÇ   ‚îú‚îÄ‚îÄ Job-Description-Dataset.csv   # Curated job descriptions with titles
‚îÇ   ‚îî‚îÄ‚îÄ Role-Resume-Dataset.csv       # Cleaned resumes and their labeled job titles
‚îú‚îÄ‚îÄ Model/
‚îÇ   ‚îî‚îÄ‚îÄ model_resume.pkl              # Trained Logistic Regression bundle (joblib)
‚îú‚îÄ‚îÄ NoteBook/
‚îÇ   ‚îú‚îÄ‚îÄ 01-Model.ipynb                # Main notebook with EDA, training, and inference helpers
‚îÇ   ‚îú‚îÄ‚îÄ README.md                     # Detailed technical documentation of the notebook
‚îÇ   ‚îî‚îÄ‚îÄ Test/test_out_put_model.py    # Prompting script that simulates an API response
‚îú‚îÄ‚îÄ output.txt                        # Example roadmap generated by the prompt script
‚îú‚îÄ‚îÄ requirements.txt                  # Python dependencies (superset of what the notebook needs)
‚îú‚îÄ‚îÄ LICENSE
‚îî‚îÄ‚îÄ README.md                         # You are here
```

> **Heads-up:** The top-level requirements file includes optional packages (e.g., transformers, torch) that were part of earlier experiments. If you only intend to run the notebook, install the "Core dependencies" listed below instead of the entire file to save disk space.

---

## üß† What the Pipeline Does

1. **Exploratory data analysis (EDA)** ‚Äì Checks for missing values, duplicate rows, class balance, word counts, and top tokens.
2. **Text preprocessing** ‚Äì Lowercasing, punctuation stripping, tokenization with `TreebankWordTokenizer`, and stop-word removal (sklearn's English list).
3. **Feature extraction** ‚Äì TF-IDF vectorization with a 5000-feature cap.
4. **Model training** ‚Äì Multiclass Logistic Regression trained on the resume vectors and encoded job titles.
5. **Evaluation** ‚Äì Accuracy plus a per-class precision/recall/F1 report (see the notebook for detailed outputs).
6. **Skill gap analysis** ‚Äì TF-IDF keyword extraction on matching job descriptions to find the most relevant yet missing terms from a resume.
7. **Model persistence** ‚Äì Saves the classifier, vectorizer, label encoder, tokenizer, stop words, and cleaned job-description frame to `Model/model_resume.pkl` using joblib.

---

## üõ†Ô∏è Core Dependencies

Install the minimal set of packages required to execute the notebook end-to-end:

```bash
pip install pandas scikit-learn nltk matplotlib seaborn joblib openai
```

If you prefer to replicate the exact development environment (including optional experimentation libraries), run:

```bash
pip install -r requirements.txt
```

> After installing, download the required NLTK corpora once:
>
> ```python
> import nltk
> nltk.download('stopwords')
> nltk.download('punkt')
> ```

---

## üöÄ Getting Started

1. **Clone the repo**
   ```bash
git clone https://github.com/Iliya-Askari/cangrow2-raw.git
cd cangrow2-raw
   ```

2. **(Optional) create and activate a virtual environment**
   ```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
   ```

3. **Install dependencies** (choose one of the options above).

4. **Download NLTK data** (only required once per machine).

5. **Open the notebook**
   ```bash
jupyter notebook NoteBook/01-Model.ipynb
   ```

---

## üìò Notebook Walk-through

The notebook is the canonical source of truth for the workflow. It is organized into the following blocks:

1. **Imports & data loading** ‚Äì Reads both CSV files and removes auto-generated index columns.
2. **EDA** ‚Äì Surface-level diagnostics, distribution plots, and keyword frequency analysis.
3. **Preprocessing helpers** ‚Äì Builds the tokenizer/stop-word pipeline and applies it to resumes and job descriptions.
4. **Vectorization & label encoding** ‚Äì Generates the TF-IDF matrix and encodes job titles.
5. **Model training & evaluation** ‚Äì Trains Logistic Regression (`max_iter=1000`) and prints accuracy + a classification report.
6. **Inference utilities** ‚Äì Includes `predict_job_and_missing_skills_v2`, which returns a predicted job title and up to 20 missing keywords for a raw resume string.
7. **Model export** ‚Äì Bundles all artifacts into `Model/model_resume.pkl` for reuse.
8. **Example inference** ‚Äì Runs the helper function on a sample resume to demonstrate the output format.

For a prose explanation of each section (including charts and rationale), consult [`NoteBook/README.md`](NoteBook/README.md).

---

## üß™ Reproducing Training Locally

You can mirror the notebook's training logic in a standard Python script. This is helpful for automation or integration tests.

```python
import joblib
import pandas as pd
import re
from nltk.tokenize import TreebankWordTokenizer
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

resumes = pd.read_csv("DataSet/Role-Resume-Dataset.csv").drop(columns=["Unnamed: 0"])
stop_words = set(ENGLISH_STOP_WORDS)
tokenizer = TreebankWordTokenizer()

def clean(text):
    text = text.lower()
    text = re.sub(r"[^a-zA-Z\s]", " ", text)
    tokens = [t for t in tokenizer.tokenize(text) if t not in stop_words]
    return " ".join(tokens)

X = resumes["resume"].apply(clean)
y = resumes["job_title"]

vectorizer = TfidfVectorizer(max_features=5000)
label_encoder = LabelEncoder()

X_vect = vectorizer.fit_transform(X)
y_encoded = label_encoder.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X_vect, y_encoded, test_size=0.2, random_state=42)
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

joblib.dump({
    "model": model,
    "vectorizer": vectorizer,
    "label_encoder": label_encoder,
    "tokenizer": tokenizer,
    "stop_words": stop_words
}, "Model/model_resume.pkl")
```

Run the script once to confirm that the environment is set up correctly and to regenerate the serialized artifact if needed.

---

## üîç Using the Saved Model

```python
import joblib
import re

bundle = joblib.load("Model/model_resume.pkl")
model = bundle["model"]
vectorizer = bundle["vectorizer"]
label_encoder = bundle["label_encoder"]
stop_words = bundle["stop_words"]
tokenizer = bundle["tokenizer"]
df_jobs = bundle.get("df_jobs")  # Optional: only present if exported from the notebook

def predict_resume(resume_text):
    cleaned = re.sub(r"[^a-zA-Z\s]", " ", resume_text.lower())
    tokens = [t for t in tokenizer.tokenize(cleaned) if t not in stop_words]
    vect = vectorizer.transform([" ".join(tokens)])
    label_id = model.predict(vect)[0]
    return label_encoder.inverse_transform([label_id])[0]

print(predict_resume("Experienced frontend engineer with React and TypeScript."))
```

If `df_jobs` is included in the bundle (it is when exported from the notebook), you can also reproduce the missing-skill logic shown in the notebook to generate keyword recommendations for the predicted role.

---

## üß™ Simulating an API-style Response

`NoteBook/Test/test_out_put_model.py` contains a minimal script that calls an OpenAI-compatible endpoint to transform the list of missing keywords into a human-readable learning roadmap. To try it yourself:

1. Set the correct `base_url` and `api_key` for your OpenAI-compatible service.
2. Run `python NoteBook/Test/test_out_put_model.py`.
3. Inspect the generated roadmap in `output.txt`.

You can adapt this script to interface with your own inference server or to format the model output for downstream applications.

---

## üìä Data Notes

- The resume dataset currently contains labelled examples spanning multiple technical roles. Review the job-title distribution plot in the notebook to understand class balance.
- The job description dataset powers the skill-gap analysis. Make sure the job titles in this file align with the resume labels to maximize keyword coverage.
- Both CSV files include an `Unnamed: 0` column created during export; the notebook drops it during preprocessing.

---

## ü§ù Contributing

1. Fork the repository and create a topic branch (e.g., `feature/improve-vectorizer`).
2. Make your changes and include appropriate documentation or notebook annotations.
3. Run the notebook or your automated script to ensure the model still trains successfully.
4. Submit a pull request describing the motivation and testing performed.

Issues and enhancement ideas are always welcome!

---

## üìÑ License & Attribution

This project is distributed under the terms of the [MIT License](LICENSE). The datasets were prepared for demonstration purposes; please ensure you have the rights to any additional data you incorporate into the pipeline.

